I want you to fully set up automated GitHub backups for my Angles AI Universe™ memory system.  
Here is the context:

- The memory system already syncs between Supabase and Notion and exports sanitized JSON data.  
- I want a GitHub backup process that runs automatically after every sync to store the latest memory data.  
- All backups must be pushed to my GitHub repository with a rotating history of commits (keep last 30 backups, delete older ones).  
- The GitHub credentials (GITHUB_TOKEN, GIT_USERNAME, GIT_EMAIL, REPO_URL) will be stored in Replit secrets.
- The backup should include:
  - The sanitized JSON exports
  - Any markdown or log files generated by the system
  - No secrets or sensitive information
- The process should:
  1. Commit and push changes if there are updates.
  2. Include timestamped commit messages like: "Backup - YYYY-MM-DD HH:MM".
  3. Run automatically after memory_bridge.py completes.
  4. Provide clear console output for success/failure.
  5. Be fully production-ready and able to run both on schedule and manually.
  6. Include a cron-scheduler in Replit that automatically runs the backup script daily at a set time (e.g., 03:00 UTC), regardless of manual runs.

Deliverables:
- A Python script (e.g., github_backup.py) that handles the backup logic.
- Integration with existing workflow so backups trigger after sync.
- Cron-scheduler in Replit to run github_backup.py daily at the specified time.
- Update README or replit.md with clear usage instructions.
- Final verification that the GitHub backup works end-to-end with my repository.

Make sure all code is clean, commented, and uses the existing project structure.

Create a file github_backup.py that:



Reads secrets GITHUB_TOKEN, GIT_USERNAME, GIT_EMAIL, REPO_URL from env.
Ensures a local git repo exists (init if missing), sets user.name/email, and sets origin to REPO_URL (replace if exists).
Exports current memory/decision data to exports/ as JSON:
From Supabase decision_vault → exports/decision_vault_YYYY-MM-DD.json
From Notion (ShoppingFriend database) → exports/notion_decisions_YYYY-MM-DD.json

Sanitizes files (remove any keys named api_key, token, secret).
Creates/updates a STATUS.md with timestamp and counts of exported rows.
Commits all changes with message: backup: memory sync {timestamp} and pushes using the PAT (use https://x-access-token:${GITHUB_TOKEN}@github.com/... for the remote).
Returns clear console output (what exported, commit SHA, push ok).
Also create run_backup_now.py that simply imports and runs main() from github_backup.py.
