Here’s the single, paste-and-run prompt for Replit AI (English, one block). It assumes your secrets are already set (SUPABASE_URL, SUPABASE_KEY, NOTION_TOKEN or NOTION_API_KEY, NOTION_DATABASE_ID, GITHUB_TOKEN, REPO_URL, GIT_USERNAME, GIT_EMAIL). It must update existing files non‑destructively and create any missing ones. Are there any and are my secrets ready and working? 

You are the Angles AI Universe™ Master Deployer. In ONE autonomous run, do the following without breaking existing working code. Keep all changes idempotent and additive (do not delete/overwrite working logic—extend it).

# 0) Dependencies & project hygiene
- Ensure/upgrade Python deps: psutil, requests, httpx, python-dotenv, supabase, pydantic, schedule, watchdog (if available), typing-extensions.
- Create/ensure `requirements.txt` includes the above and pin reasonable versions. Install.
- Ensure folders exist: logs/, logs/active/, logs/archive/, export/, utils/, tests/.
- Add a root `.env.example` showing all required env vars (no real values).

# 1) Database migration (idempotent)
Create/refresh a script `run_migration.py` that:
- Verifies Supabase connection using SUPABASE_URL/SUPABASE_KEY.
- Ensures tables/columns (create if not exists):
  • decision_vault(id uuid pk default gen_random_uuid(), decision text not null, date date default current_date, type text not null, active boolean not null default true, comment text, synced boolean not null default false, synced_at timestamptz, created_at timestamptz default now()).
  • agent_activity(id bigserial pk, agent_name text, action text, details text, status text, created_at timestamptz default now()).
- Ensures indexes: idx_decision_date on decision_vault(date), idx_agent_name on agent_activity(agent_name).
- Exit with clear printouts of any changes applied.

# 2) Memory sync (Supabase ⇄ Notion) – unify/upgrade
Create/refresh `memory_sync.py` that:
- Reads unsynced rows from decision_vault (synced=false).
- Creates/updates corresponding pages in Notion database (NOTION_TOKEN or NOTION_API_KEY + NOTION_DATABASE_ID). Properties to use: Name/title=decision, Date=date, Tag=type (multi-select), Comment=comment.
- After success, PATCH the row: synced=true, synced_at=now().
- Writes structured log lines to `logs/active/memory_sync.log` and a JSON summary to `export/decisions_YYYYMMDD.json`.
- Provide CLI flags:
  • `--test` (connection test only)
  • `--all` (ignore synced flag, resync everything)
  • `--recent` (limit 24h)
  • `--dry-run`
- Reuse any existing helpers if present; otherwise create a small Notion client using httpx.

# 3) AutoSync for file changes → DecisionVault
Create `autosync_files.py` that:
- Scans the repo (exclude .git, logs/, __pycache__, venv) and computes file hashes.
- On first run creates `export/file_manifest.json`.
- On subsequent runs detects changed files and logs an entry into `agent_activity` with agent_name="autosync", action="file_change", details JSON {path, hash, ts}.
- Optionally create a minimal “content snapshot” row into decision_vault for key files under /operations.py, /architects_table_model.py, /memory_bridge.py if changed (decision="File updated: <path>", type="technical", comment="autosync").
- Write progress to `logs/active/autosync.log`.
- Provide flags: `--once`, `--watch` (poll every 60s), `--dry-run`.

# 4) Health monitoring & self-healing
Create/refresh `backend_monitor.py` that performs:
- Env validation: prints ✓/✗ for SUPABASE, NOTION, GITHUB secrets.
- Dependency check: report missing/outdated packages.
- Service checks: can import and run `run_migration.py --dry-run`, `memory_sync.py --test`, and a light `autosync_files.py --once`.
- System metrics via psutil (CPU, RAM, disk). Threshold warnings.
- GitHub reachability check using REPO_URL auth.
- Writes report to `logs/active/system_health.log` and a JSON version to `logs/active/system_health.json`.
- Exit non‑zero on critical failures.

# 5) Backup & restore (GitHub)
- If not already present, create `utils/git_helpers.py` with functions: init_repo_if_needed(), set_identity(GIT_USERNAME,GIT_EMAIL), commit_and_push(paths:list, message), safe_pull_with_rebase().
- Create/refresh `run_backup_now.py`:
  • Sanitize `export/*.json` (no secrets) and copy to `export/safe/`.
  • Commit/push only `export/safe/*.json` and `logs/memory_sync.log` (if allowed) to REPO_URL using GITHUB_TOKEN; respect .gitignore (never push .env or logs/** except allowed files).
  • Handle divergent branches (auto fetch, rebase or fast-forward; on conflict, write warning and skip).
- Create/refresh `restore_from_github.py`:
  • Pull latest from REPO_URL and allow restoring `export/safe/*.json` back into Supabase (option `--restore-decisions`) with collision-safe upserts (matching by decision text + date).

# 6) Unified scheduler (no external cron)
Create `scheduler.py` that runs as a long-lived process using `schedule`:
- Every hour: `python backend_monitor.py`
- Every 6 hours: `python memory_sync.py`
- Daily 02:00 UTC: `python run_backup_now.py`
- Sunday 02:00 UTC: `python restore_from_github.py --dry-run` (verification)
- Optional: if watchdog is available, start `autosync_files.py --watch`
- All stdout/stderr go to `logs/active/scheduler.log` (rotate daily to logs/archive).

# 7) Tests & verification
Create `tests/test_all.py` that:
- Imports each script and runs safe “test/dry_run” mode.
- Asserts non‑error return codes and basic side-effects (e.g., health JSON exists, export JSON exists).
- Prints a final PASS/FAIL summary.
Add a convenience runner `run_all.py` that:
  • Runs: run_migration (dry), memory_sync --test, autosync_files --once, backend_monitor, tests/test_all.py
  • Prints a compact “ALL SYSTEMS OK” summary with timestamps.

# 8) Documentation & quick commands
- Create/refresh `README_BACKEND.md` with:
  • What each script does, required env vars, how to run manually.
  • “First-time setup” section and troubleshooting.
- Add a one-page `OPERATIONS.md` with runbook: common commands, where logs are, how to restore, how to rotate logs.

# 9) Final execution & output
- Run `python run_migration.py` (full).
- Run `python backend_monitor.py`.
- Run `python memory_sync.py --recent`.
- Start the scheduler in the background (if Replit supports background tab) OR print the exact command for me to run.
- Return a final section titled: “✅ Master deployment complete” including:
  • Confirmed versions of installed packages
  • Next manual commands I can use:
    - `python scheduler.py`
    - `python memory_sync.py --all`
    - `python autosync_files.py --once`
    - `python run_backup_now.py`
    - `python restore_from_github.py --dry-run`
    - `python tests/test_all.py`
  • Any warnings or TODOs you detected.

Constraints:
- Preserve existing working files (memory_bridge.py, git backup/restore scripts, etc.). If a function already exists, extend it rather than replace.
- All scripts must use English logs/messages.
- Use robust error handling and clear logging.
- Keep secrets out of exports and git.